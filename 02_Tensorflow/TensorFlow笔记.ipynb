{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B站：北大TensorFlow笔记\n",
    "\n",
    "## 目标1：\n",
    "搭建神经网络，总结搭建的八股\n",
    "### TensorFlow中的神经网络\n",
    "\n",
    "#### 整个计算过程\n",
    "* 张量表示数据\n",
    "* 计算图搭建神经网络\n",
    "* 会话执行计算图\n",
    "* 优化线上权重（参数）\n",
    "* 得到模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量表示数据\n",
    "1.先用`constant`构建两个常数向量的变量  \n",
    "2.两个向量做加法，返回值\n",
    "\n",
    "关于相应的名称\n",
    "* 乘法：mul\n",
    "* 除法：truediv\n",
    "* 加法：add\n",
    "* 减法：sub\n",
    "* 取余数：mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mod:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.0, 2.0])\n",
    "b = tf.constant([3.0, 4.0])\n",
    "result = a % b\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算图（Graph）搭建神经网路\n",
    "\n",
    "**能print()出以下内容的变量都称为计算图，即有加减乘除内容的运算**  \n",
    "`Tensor(\"mod:0\", shape=(2,), dtype=float32)`\n",
    "\n",
    "* 计算图参数的解读：  \n",
    "    * 张量名称`add:0`\n",
    "        * add->节点名  \n",
    "        * 0->第0个输出  \n",
    "    * shape->维度（2,）表示一维数组，长度为2  \n",
    "    * dtype=float32->数据类型\n",
    "\n",
    "**图只负责搭建网络，不进行计算**\n",
    "\n",
    "**矩阵相乘案例**\n",
    "\n",
    "* 一个中括号内有几个数，就代表有几列数\n",
    "* 一个中括号内有几个中括号，就代表有几行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_6:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.0, 2.0]])\n",
    "w = tf.constant([[3.0], [4.0]])\n",
    "y = tf.matmul(x, w)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用会话（Session）进行计算，执行计算图\n",
    "使用with实现，结构如下\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(1, 1), dtype=float32)\n",
      "[[ 11.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.0, 2.0]])\n",
    "w = tf.constant([[3.0], [4.0]])\n",
    "y = tf.matmul(x, w)\n",
    "print(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化权重（参数）\n",
    "\n",
    "**1.首先随机生成参数`tf.Variable()`**  \n",
    "`w = tf.Variable(tf.random_normal([2,3], stddev=2, mean=0, seed=1))`\n",
    "\n",
    "* tf.random_normal：生成正态分布的随机数\n",
    "    * 大小[2,3]\n",
    "    * 以下没有特殊要求可以不写：\n",
    "        * stddev：标准差=2\n",
    "        * mean：均值为0\n",
    "        * seed：随机种子为1（表示做随机数的序列，固定随机种子，每次生成的随机数保持一致）\n",
    "\n",
    "**2.关于生成随机数的分布，有以下几种：**  \n",
    "* tf.random_normal()：正态分布\n",
    "* tf.truncated_normal()：去掉过大偏离点的正态分布（如果随机出的数据超出平均值两个标准差，则会被去掉）\n",
    "* tf.random_uniform()：平均分布\n",
    "\n",
    "**3.生成数组常量**\n",
    "* tf.zeros，生成全0数组，`tf.zeros([3,2],int32)`，生成三行两列全0矩阵\n",
    "* tf.ones，生成全1数组，`tf.ones([3,2],int32)`，生成三行两列全1矩阵\n",
    "* tf.fill，生成全定值数组，`tf.fill([3,2],6)`，生成三行两列全1矩阵\n",
    "* tf.constant，根据给定的数值生成数组（前面用到过）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 6]\n",
      " [6 6]\n",
      " [6 6]]\n"
     ]
    }
   ],
   "source": [
    "q = tf.zeros([3,2], tf.int32)\n",
    "q = tf.ones([3,2], tf.int32)\n",
    "q = tf.fill([3,2], 6)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络的实现\n",
    "\n",
    "**整个过程：**\n",
    "\n",
    "* 训练模型  \n",
    "1.准备数据集，提取特征，作为输入喂给NN  \n",
    "2.从输入到输出，搭建NN结构（前向传播）：先搭建计算图，再执行会话  \n",
    "3.将特征数据喂给NN，迭代优化NN权值参数（反向传播） \n",
    "\n",
    "\n",
    "* 使用模型  \n",
    "4.将训练好的模型进行预测和分类  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建前向传播\n",
    "\n",
    "#### 变量初始化\n",
    "* 1.定义变量初值（搭建框架）\n",
    "初始化所有变量的函数，简写为初始化节点`init_op`\n",
    "```\n",
    "x = tf.constant([[0.7, 0.5]])\n",
    "w1 = tf.Variable(tf.random_normal([2, 3],stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1],stddev=1, seed=1))\n",
    "```\n",
    "* 2.执行初始化（赋予变量初值）\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "```\n",
    "\n",
    "#### 张量乘法\n",
    "* 1.定义张量之间的关系（搭建框架）\n",
    "```\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "```\n",
    "* 2.执行乘法\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    print(\"y is:\\n\", sess.run(y))\n",
    "```\n",
    "\n",
    "\n",
    "#### Other Points\n",
    "1.参数矩阵的具体形状  \n",
    "* 本层指的是计算层（W1是指第一个计算层，W2指的是第二层，因此不包含输入层）  \n",
    "* (上层节点个数, 本层节点个数）\n",
    "\n",
    "2.当前为什么没有用到激活函数和偏置值b？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is:\n",
      " [[ 3.0904665]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 两层全连接神经网络demo\n",
    "\n",
    "# 1.定义输入、随机正态分布初始化权重参数的初值\n",
    "# 定义一个固定输入，使用tf.constant\n",
    "x = tf.constant([[0.7, 0.5]])\n",
    "w1 = tf.Variable(tf.random_normal([2, 3],stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1],stddev=1, seed=1))\n",
    "\n",
    "# 2.定义前向传播过程中变量之间的关系（相乘）\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 3.使用Session计算结果\n",
    "with tf.Session() as sess:\n",
    "    # 01.初始化变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 02.进行NN计算，并且输出结果\n",
    "    print(\"y is:\\n\", sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 构建反向传播\n",
    "\n",
    "#### 喂数据\n",
    "先用`tf.placeholder`进行占位，shape=(数据个数, 数据维度)，告诉`sess.run(y)`要输入多少个数据\n",
    "\n",
    "* 喂一组数据，数据有两个特征：\n",
    "先占一个位置的数据，然后输入数据\n",
    "```\n",
    "x = tf.placeholder(tf.float32,shape=(1, 2))\n",
    "sess.run(y, feed_dict={x:[[0.5, 0.6]]})\n",
    "```\n",
    "\n",
    "* 喂多组数据，数据同样是有两维特征：\n",
    "数据个数填写None，代表无数个数据/未知\n",
    "```\n",
    "x = tf.placeholder(tf.float32,shape=(None, 2))\n",
    "sess.run(y, feed_dict={x:[[0.1,0.2], [0.2,0.3], [0.3,0.4], [0.4,0.5]]})\n",
    "```\n",
    "\n",
    "##### 向NN喂入一组特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is:\n",
      " [[ 3.0904665]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 两层全连接神经网络demo\n",
    "\n",
    "# 1.定义输入、随机正态分布初始化权重参数的初值\n",
    "# 定义一个输入，使用tf.placeholder()\n",
    "x = tf.placeholder(tf.float32, shape=(1, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3],stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1],stddev=1, seed=1))\n",
    "\n",
    "# 2.定义前向传播过程中变量之间的关系（相乘）\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 3.使用Session计算结果\n",
    "with tf.Session() as sess:\n",
    "    # 01.初始化变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 02.使用feed_dict喂入参数，进行NN计算，并且输出结果\n",
    "    print(\"y is:\\n\", sess.run(y, feed_dict={x: [[0.7, 0.5]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 向NN喂入多组特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is:\n",
      " [[ 3.0904665 ]\n",
      " [ 1.2236414 ]\n",
      " [ 1.72707319]\n",
      " [ 2.23050475]]\n",
      "w1:\n",
      " [[-0.81131822  1.48459876  0.06532937]\n",
      " [-2.4427042   0.0992484   0.59122431]]\n",
      "w2:\n",
      " [[-0.81131822]\n",
      " [ 1.48459876]\n",
      " [ 0.06532937]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 两层全连接神经网络demo\n",
    "\n",
    "# 1.定义输入、随机正态分布初始化权重参数的初值\n",
    "# 定义多个输入，使用tf.placeholder()\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3],stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1],stddev=1, seed=1))\n",
    "\n",
    "# 2.定义前向传播过程中变量之间的关系（相乘）\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 3.使用Session计算结果\n",
    "with tf.Session() as sess:\n",
    "    # 01.初始化变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 02.使用feed_dict喂入参数，进行NN计算，并且输出结果\n",
    "    print(\"y is:\\n\", sess.run(y, feed_dict={x: [[0.7, 0.5], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5]]}))\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 损失函数\n",
    "常见的损失函数\n",
    "* MSE均方误差\n",
    "即为预测值与真实值差的平方，除以样本个数n  \n",
    "`loss = tf.reduce_mean(tf.square(y_-y))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 反向传播训练\n",
    "目标：减小loss数值  \n",
    "常见方法：\n",
    "* 梯度下降  \n",
    "`train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)`\n",
    "* Momentum优化  \n",
    "`train_step = tf.train.MomentumOptimizer(learining_rate, momentum).minimize(loss)`\n",
    "* Adam  \n",
    "`train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)`\n",
    "\n",
    "其中，学习率learning_rate代表参数每次更新的幅度\n",
    "\n",
    "**代码案例如下：需要背下来**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch_size：一次喂给神经网络多少数据（此数值不可以过大，否则会吃不消）\n",
    "BATCH_SIZE = 8\n",
    "seed = 23455\n",
    "\n",
    "# 基于随机种子23455生成数据集\n",
    "rng = np.random.RandomState(seed)\n",
    "# 生成32行数据，每组数据都有 体积和重量 两个属性作为特征\n",
    "X = rng.rand(32,2)\n",
    "Y = [[int(x0 + x1 < 1)] for (x0, x1) in X]\n",
    "\n",
    "print(\"X:\\n\",X)\n",
    "print(\"Y:\\n\",Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 2.搭建NN的输入、输出、参数\n",
    "* 3.搭建NN的前向传播过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 搭建NN的输入、输出、参数（其中y是矩阵计算后的值，而y_是从矩阵Y中取出来的标签\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_= tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 搭建NN的前向传播过程\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 4.反向传播，指定损失函数loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 均方误差计算损失\n",
    "loss = tf.reduce_mean(tf.square(y-y_))\n",
    "\n",
    "# 梯度下降开始学习，学习率0.001\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "train_step = tf.train.MomentumOptimizer(0.001, 0.9).minimize(loss)\n",
    "# train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成Session，训练steps轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[-0.81131822  1.48459876  0.06532937]\n",
      " [-2.4427042   0.0992484   0.59122431]]\n",
      "w2:\n",
      " [[-0.81131822]\n",
      " [ 1.48459876]\n",
      " [ 0.06532937]]\n",
      "\n",
      "\n",
      "After 1 training steps, loss_mse on all data is 5.13118\n",
      "After 501 training steps, loss_mse on all data is 0.384391\n",
      "After 1001 training steps, loss_mse on all data is 0.383592\n",
      "After 1501 training steps, loss_mse on all data is 0.383562\n",
      "After 2001 training steps, loss_mse on all data is 0.383561\n",
      "After 2501 training steps, loss_mse on all data is 0.383561\n",
      "After 3001 training steps, loss_mse on all data is 0.383561\n",
      "After 3501 training steps, loss_mse on all data is 0.383561\n",
      "After 4001 training steps, loss_mse on all data is 0.383561\n",
      "After 4501 training steps, loss_mse on all data is 0.383561\n",
      "After 5001 training steps, loss_mse on all data is 0.383561\n",
      "After 5501 training steps, loss_mse on all data is 0.383561\n",
      "After 6001 training steps, loss_mse on all data is 0.383561\n",
      "After 6501 training steps, loss_mse on all data is 0.383561\n",
      "After 7001 training steps, loss_mse on all data is 0.383561\n",
      "After 7501 training steps, loss_mse on all data is 0.383561\n",
      "After 8001 training steps, loss_mse on all data is 0.383561\n",
      "After 8501 training steps, loss_mse on all data is 0.383561\n",
      "After 9001 training steps, loss_mse on all data is 0.383561\n",
      "After 9501 training steps, loss_mse on all data is 0.383561\n",
      "After 10001 training steps, loss_mse on all data is 0.383561\n",
      "After 10501 training steps, loss_mse on all data is 0.383561\n",
      "After 11001 training steps, loss_mse on all data is 0.383561\n",
      "After 11501 training steps, loss_mse on all data is 0.383561\n",
      "After 12001 training steps, loss_mse on all data is 0.383561\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 初始化变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 输出训练前的权重\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # 训练模型\n",
    "    STEPS = 12001\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        # 分批喂入训练数据，进行权重的学习（梯度下降法）\n",
    "        sess.run(train_step, feed_dict={x:X[start:end], y_:Y[start:end]})\n",
    "#         print(\"train_step\\n\",train_step)\n",
    "        \n",
    "        # 输出训练后的权值参数\n",
    "#         print(\"\\n\")\n",
    "#         print(\"w1:\\n\",sess.run(w1))\n",
    "#         print(\"w2:\\n\",sess.run(w2))\n",
    "        \n",
    "        # 每500次计算一次均方误差\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss, feed_dict={x:X, y_:Y})\n",
    "            print(\"After %d training steps, loss_mse on all data is %g\" % (i+1, total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标2：优化神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 损失函数 loss\n",
    "* 目标：使得损失loss最小\n",
    "* 常见方法：\n",
    "    * 均方误差MSE(Mean Squared Error)  \n",
    "    `loss_mse = tf.reduce_mean(tf.square(y_-y))`\n",
    "    * 自定义\n",
    "    * 交叉熵CE(Cross Entropy)\n",
    "    \n",
    "#### 均方误差MSE案例\n",
    "例：建模牛奶的销量，以预测牛奶的产量  \n",
    "本案例只有一个输出层，即单层神经网络  \n",
    "本案例中，默认预测多了与预测少了损失相同。（但是实际情况，预测多了损失成本，预测少了损失利润）  \n",
    "预期生成的应该是X1+X2，实际生成的是0.98X1+1.02X2，说明预测酸奶日销量的函数拟合正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training steps, w1 is: \n",
      "[[-0.80974597]\n",
      " [ 1.48529029]] \n",
      "\n",
      "After 501 training steps, w1 is: \n",
      "[[-0.46074435]\n",
      " [ 1.64187801]] \n",
      "\n",
      "After 1001 training steps, w1 is: \n",
      "[[-0.21939856]\n",
      " [ 1.69847655]] \n",
      "\n",
      "After 1501 training steps, w1 is: \n",
      "[[-0.04415595]\n",
      " [ 1.70031762]] \n",
      "\n",
      "After 2001 training steps, w1 is: \n",
      "[[ 0.08942621]\n",
      " [ 1.67332804]] \n",
      "\n",
      "After 2501 training steps, w1 is: \n",
      "[[ 0.19583553]\n",
      " [ 1.63226771]] \n",
      "\n",
      "After 3001 training steps, w1 is: \n",
      "[[ 0.28375748]\n",
      " [ 1.58544338]] \n",
      "\n",
      "After 3501 training steps, w1 is: \n",
      "[[ 0.35848638]\n",
      " [ 1.53744709]] \n",
      "\n",
      "After 4001 training steps, w1 is: \n",
      "[[ 0.42332521]\n",
      " [ 1.49073923]] \n",
      "\n",
      "After 4501 training steps, w1 is: \n",
      "[[ 0.48040032]\n",
      " [ 1.44655728]] \n",
      "\n",
      "After 5001 training steps, w1 is: \n",
      "[[ 0.5311361 ]\n",
      " [ 1.40545344]] \n",
      "\n",
      "After 5501 training steps, w1 is: \n",
      "[[ 0.57653254]\n",
      " [ 1.367594  ]] \n",
      "\n",
      "After 6001 training steps, w1 is: \n",
      "[[ 0.6173259 ]\n",
      " [ 1.33294022]] \n",
      "\n",
      "After 6501 training steps, w1 is: \n",
      "[[ 0.65408474]\n",
      " [ 1.30134249]] \n",
      "\n",
      "After 7001 training steps, w1 is: \n",
      "[[ 0.68726856]\n",
      " [ 1.27260184]] \n",
      "\n",
      "After 7501 training steps, w1 is: \n",
      "[[ 0.71725982]\n",
      " [ 1.24650037]] \n",
      "\n",
      "After 8001 training steps, w1 is: \n",
      "[[ 0.74438614]\n",
      " [ 1.22281957]] \n",
      "\n",
      "After 8501 training steps, w1 is: \n",
      "[[ 0.76893252]\n",
      " [ 1.20134819]] \n",
      "\n",
      "After 9001 training steps, w1 is: \n",
      "[[ 0.79115146]\n",
      " [ 1.18188882]] \n",
      "\n",
      "After 9501 training steps, w1 is: \n",
      "[[ 0.81126714]\n",
      " [ 1.16425669]] \n",
      "\n",
      "After 10001 training steps, w1 is: \n",
      "[[ 0.82948142]\n",
      " [ 1.14828289]] \n",
      "\n",
      "After 10501 training steps, w1 is: \n",
      "[[ 0.84597576]\n",
      " [ 1.13381267]] \n",
      "\n",
      "After 11001 training steps, w1 is: \n",
      "[[ 0.8609128 ]\n",
      " [ 1.12070608]] \n",
      "\n",
      "After 11501 training steps, w1 is: \n",
      "[[ 0.87444043]\n",
      " [ 1.10883462]] \n",
      "\n",
      "After 12001 training steps, w1 is: \n",
      "[[ 0.88669145]\n",
      " [ 1.09808242]] \n",
      "\n",
      "After 12501 training steps, w1 is: \n",
      "[[ 0.89778632]\n",
      " [ 1.08834386]] \n",
      "\n",
      "After 13001 training steps, w1 is: \n",
      "[[ 0.90783483]\n",
      " [ 1.07952428]] \n",
      "\n",
      "After 13501 training steps, w1 is: \n",
      "[[ 0.91693527]\n",
      " [ 1.0715363 ]] \n",
      "\n",
      "After 14001 training steps, w1 is: \n",
      "[[ 0.92517716]\n",
      " [ 1.06430185]] \n",
      "\n",
      "After 14501 training steps, w1 is: \n",
      "[[ 0.93264157]\n",
      " [ 1.05774975]] \n",
      "\n",
      "After 15001 training steps, w1 is: \n",
      "[[ 0.93940228]\n",
      " [ 1.05181527]] \n",
      "\n",
      "After 15501 training steps, w1 is: \n",
      "[[ 0.94552511]\n",
      " [ 1.0464406 ]] \n",
      "\n",
      "After 16001 training steps, w1 is: \n",
      "[[ 0.95107025]\n",
      " [ 1.04157281]] \n",
      "\n",
      "After 16501 training steps, w1 is: \n",
      "[[ 0.95609277]\n",
      " [ 1.03716397]] \n",
      "\n",
      "After 17001 training steps, w1 is: \n",
      "[[ 0.96064115]\n",
      " [ 1.03317142]] \n",
      "\n",
      "After 17501 training steps, w1 is: \n",
      "[[ 0.96476096]\n",
      " [ 1.02955461]] \n",
      "\n",
      "After 18001 training steps, w1 is: \n",
      "[[ 0.96849167]\n",
      " [ 1.02628016]] \n",
      "\n",
      "After 18501 training steps, w1 is: \n",
      "[[ 0.97187072]\n",
      " [ 1.02331424]] \n",
      "\n",
      "After 19001 training steps, w1 is: \n",
      "[[ 0.974931  ]\n",
      " [ 1.02062762]] \n",
      "\n",
      "After 19501 training steps, w1 is: \n",
      "[[ 0.97770262]\n",
      " [ 1.01819491]] \n",
      "\n",
      "Final w1 is: \n",
      " [[ 0.98019385]\n",
      " [ 1.01598072]]\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "# 导入模块\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 生成数据集\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "# 这里虽数据加入了噪声: -0.05 ~ +0.05\n",
    "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "\n",
    "# 定义神经网络输入、输出、权重，定义前向传播过程\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))\n",
    "y = tf.matmul(x,w1)\n",
    "\n",
    "# 定义损失函数MSE & 反向传播（方法为梯度下降法）\n",
    "loss_mse = tf.reduce_mean(tf.square(y_-y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "# train_step = tf.train.MomentumOptimizer(0.001, 0.9).minimize(loss_mse)\n",
    "# train_step = tf.train.AdamOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "# 生成Session，启动训练，进行STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 20000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_:Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i+1))\n",
    "            print(sess.run(w1),\"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义损失案例\n",
    "\n",
    "考虑成本和损失不相同，使用**自定义损失函数**来计算\n",
    "\n",
    "制定分段函数：\n",
    "预测多了，损失成本；预测少了，损失利润（单件成本1元，利润9元），因此生产越多越好（如果利润是1，成本是9，那么NN会朝着尽量少生产的方向走）  \n",
    "使用`greater()`函数判断两个变量的大小，最后将损失求和  \n",
    "`loss = tf.reduce_sum(tf.where(tf.greater(y,y_)，COST(y-y_), PROFIT(y_-y)))`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training steps, w1 is: \n",
      "[[-0.80594873]\n",
      " [ 1.48737288]] \n",
      "\n",
      "After 501 training steps, w1 is: \n",
      "[[ 0.8732146 ]\n",
      " [ 1.00620401]] \n",
      "\n",
      "After 1001 training steps, w1 is: \n",
      "[[ 0.96580642]\n",
      " [ 0.96982086]] \n",
      "\n",
      "After 1501 training steps, w1 is: \n",
      "[[ 0.96454471]\n",
      " [ 0.96829468]] \n",
      "\n",
      "After 2001 training steps, w1 is: \n",
      "[[ 0.96024752]\n",
      " [ 0.97420847]] \n",
      "\n",
      "After 2501 training steps, w1 is: \n",
      "[[ 0.96100295]\n",
      " [ 0.96993423]] \n",
      "\n",
      "After 3001 training steps, w1 is: \n",
      "[[ 0.96541017]\n",
      " [ 0.97611594]] \n",
      "\n",
      "After 3501 training steps, w1 is: \n",
      "[[ 0.96414846]\n",
      " [ 0.97458977]] \n",
      "\n",
      "After 4001 training steps, w1 is: \n",
      "[[ 0.95985126]\n",
      " [ 0.98050356]] \n",
      "\n",
      "After 4501 training steps, w1 is: \n",
      "[[ 0.96364218]\n",
      " [ 0.96878934]] \n",
      "\n",
      "After 5001 training steps, w1 is: \n",
      "[[ 0.95934498]\n",
      " [ 0.97470313]] \n",
      "\n",
      "After 5501 training steps, w1 is: \n",
      "[[ 0.9667877 ]\n",
      " [ 0.97344488]] \n",
      "\n",
      "After 6001 training steps, w1 is: \n",
      "[[ 0.96419948]\n",
      " [ 0.96766263]] \n",
      "\n",
      "After 6501 training steps, w1 is: \n",
      "[[ 0.96557122]\n",
      " [ 0.98128432]] \n",
      "\n",
      "After 7001 training steps, w1 is: \n",
      "[[ 0.96530831]\n",
      " [ 0.98170197]] \n",
      "\n",
      "After 7501 training steps, w1 is: \n",
      "[[ 0.96708208]\n",
      " [ 0.97273582]] \n",
      "\n",
      "After 8001 training steps, w1 is: \n",
      "[[ 0.96480203]\n",
      " [ 0.97590154]] \n",
      "\n",
      "After 8501 training steps, w1 is: \n",
      "[[ 0.96586561]\n",
      " [ 0.98057526]] \n",
      "\n",
      "After 9001 training steps, w1 is: \n",
      "[[ 0.9646039 ]\n",
      " [ 0.97904909]] \n",
      "\n",
      "After 9501 training steps, w1 is: \n",
      "[[ 0.96303403]\n",
      " [ 0.96857494]] \n",
      "\n",
      "After 10001 training steps, w1 is: \n",
      "[[ 0.95873684]\n",
      " [ 0.97448874]] \n",
      "\n",
      "After 10501 training steps, w1 is: \n",
      "[[ 0.95980042]\n",
      " [ 0.97916245]] \n",
      "\n",
      "After 11001 training steps, w1 is: \n",
      "[[ 0.96693498]\n",
      " [ 0.96895623]] \n",
      "\n",
      "After 11501 training steps, w1 is: \n",
      "[[ 0.95929414]\n",
      " [ 0.97336203]] \n",
      "\n",
      "After 12001 training steps, w1 is: \n",
      "[[ 0.96004957]\n",
      " [ 0.96908778]] \n",
      "\n",
      "After 12501 training steps, w1 is: \n",
      "[[ 0.96009481]\n",
      " [ 0.9784534 ]] \n",
      "\n",
      "After 13001 training steps, w1 is: \n",
      "[[ 0.96085024]\n",
      " [ 0.97417915]] \n",
      "\n",
      "After 13501 training steps, w1 is: \n",
      "[[ 0.96494931]\n",
      " [ 0.9714129 ]] \n",
      "\n",
      "After 14001 training steps, w1 is: \n",
      "[[ 0.96499455]\n",
      " [ 0.98077852]] \n",
      "\n",
      "After 14501 training steps, w1 is: \n",
      "[[ 0.96373284]\n",
      " [ 0.97925234]] \n",
      "\n",
      "After 15001 training steps, w1 is: \n",
      "[[ 0.96216297]\n",
      " [ 0.96877819]] \n",
      "\n",
      "After 15501 training steps, w1 is: \n",
      "[[ 0.96019107]\n",
      " [ 0.98089188]] \n",
      "\n",
      "After 16001 training steps, w1 is: \n",
      "[[ 0.95892936]\n",
      " [ 0.97936571]] \n",
      "\n",
      "After 16501 training steps, w1 is: \n",
      "[[ 0.96606392]\n",
      " [ 0.96915948]] \n",
      "\n",
      "After 17001 training steps, w1 is: \n",
      "[[ 0.96044022]\n",
      " [ 0.97081721]] \n",
      "\n",
      "After 17501 training steps, w1 is: \n",
      "[[ 0.96484745]\n",
      " [ 0.97699893]] \n",
      "\n",
      "After 18001 training steps, w1 is: \n",
      "[[ 0.96427637]\n",
      " [ 0.96846861]] \n",
      "\n",
      "After 18501 training steps, w1 is: \n",
      "[[ 0.95997918]\n",
      " [ 0.9743824 ]] \n",
      "\n",
      "After 19001 training steps, w1 is: \n",
      "[[ 0.9630599 ]\n",
      " [ 0.97630805]] \n",
      "\n",
      "After 19501 training steps, w1 is: \n",
      "[[ 0.96715897]\n",
      " [ 0.9735418 ]] \n",
      "\n",
      "Final w1 is: \n",
      " [[ 0.96619672]\n",
      " [ 0.97694939]]\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "# 导入模块\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 生成数据集\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "COST = 1\n",
    "PROFIT = 9\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32,2)\n",
    "# 这里虽数据加入了噪声: -0.05 ~ +0.05\n",
    "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "\n",
    "# 定义神经网络输入、输出、权重，定义前向传播过程\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1 = tf.Variable(tf.random_normal([2,1], stddev=1, seed=1))\n",
    "y = tf.matmul(x,w1)\n",
    "\n",
    "# 自定义损失函数 & 反向传播（方法为梯度下降法）\n",
    "loss_mse = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_)*COST, (y_ - y)*PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "# train_step = tf.train.MomentumOptimizer(0.001, 0.9).minimize(loss_mse)\n",
    "# train_step = tf.train.AdamOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "# 生成Session，启动训练，进行STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 20000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_:Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d training steps, w1 is: \" % (i+1))\n",
    "            print(sess.run(w1),\"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉熵CE案例\n",
    "交叉熵：表征两个概率分布的距离（交叉熵越大，两个概率分布越远，交叉熵越小，两个概率分布越近）\n",
    "\n",
    "工程中不会用到以下函数：\n",
    "`ce = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y, 1e-12, 1.0)))`\n",
    "\n",
    "![](https://raw.githubusercontent.com/king0682/FigureBed/master/img/20190718225630.png)\n",
    "\n",
    "**softmax函数目的：**  \n",
    "1.为了让前向传播的结果y满足概率分布，让n分类的n个输出，每个都在[0,1]之间  \n",
    "2.且n个输出之和为1  \n",
    "\n",
    "![](https://raw.githubusercontent.com/king0682/FigureBed/master/img/20190718230434.png)\n",
    "\n",
    "**实际操作：**\n",
    "\n",
    "1.输出经过softmax函数，使其满足概率分布  \n",
    "2.将输出结果与标准答案求交叉熵  \n",
    "\n",
    "工程中用到以下两句函数(cem是损失函数，在后面手写数字识别会用)：\n",
    "```\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "cem = tf.reduce_mean(ce)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率 learning rate\n",
    "![](https://raw.githubusercontent.com/king0682/FigureBed/master/img/20190718231738.png)\n",
    "\n",
    "**案例：**  \n",
    "损失函数loss = (w+1)^2, w初值为5，反向传播找最优w，即对应loss最小的w值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 steps: w is 2.600000, loss is 12.959999.\n",
      "After 1 steps: w is 1.160000, loss is 4.665599.\n",
      "After 2 steps: w is 0.296000, loss is 1.679616.\n",
      "After 3 steps: w is -0.222400, loss is 0.604662.\n",
      "After 4 steps: w is -0.533440, loss is 0.217678.\n",
      "After 5 steps: w is -0.720064, loss is 0.078364.\n",
      "After 6 steps: w is -0.832038, loss is 0.028211.\n",
      "After 7 steps: w is -0.899223, loss is 0.010156.\n",
      "After 8 steps: w is -0.939534, loss is 0.003656.\n",
      "After 9 steps: w is -0.963720, loss is 0.001316.\n",
      "After 10 steps: w is -0.978232, loss is 0.000474.\n",
      "After 11 steps: w is -0.986939, loss is 0.000171.\n",
      "After 12 steps: w is -0.992164, loss is 0.000061.\n",
      "After 13 steps: w is -0.995298, loss is 0.000022.\n",
      "After 14 steps: w is -0.997179, loss is 0.000008.\n",
      "After 15 steps: w is -0.998307, loss is 0.000003.\n",
      "After 16 steps: w is -0.998984, loss is 0.000001.\n",
      "After 17 steps: w is -0.999391, loss is 0.000000.\n",
      "After 18 steps: w is -0.999634, loss is 0.000000.\n",
      "After 19 steps: w is -0.999781, loss is 0.000000.\n",
      "After 20 steps: w is -0.999868, loss is 0.000000.\n",
      "After 21 steps: w is -0.999921, loss is 0.000000.\n",
      "After 22 steps: w is -0.999953, loss is 0.000000.\n",
      "After 23 steps: w is -0.999972, loss is 0.000000.\n",
      "After 24 steps: w is -0.999983, loss is 0.000000.\n",
      "After 25 steps: w is -0.999990, loss is 0.000000.\n",
      "After 26 steps: w is -0.999994, loss is 0.000000.\n",
      "After 27 steps: w is -0.999996, loss is 0.000000.\n",
      "After 28 steps: w is -0.999998, loss is 0.000000.\n",
      "After 29 steps: w is -0.999999, loss is 0.000000.\n",
      "After 30 steps: w is -0.999999, loss is 0.000000.\n",
      "After 31 steps: w is -1.000000, loss is 0.000000.\n",
      "After 32 steps: w is -1.000000, loss is 0.000000.\n",
      "After 33 steps: w is -1.000000, loss is 0.000000.\n",
      "After 34 steps: w is -1.000000, loss is 0.000000.\n",
      "After 35 steps: w is -1.000000, loss is 0.000000.\n",
      "After 36 steps: w is -1.000000, loss is 0.000000.\n",
      "After 37 steps: w is -1.000000, loss is 0.000000.\n",
      "After 38 steps: w is -1.000000, loss is 0.000000.\n",
      "After 39 steps: w is -1.000000, loss is 0.000000.\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义待优化参数w初值为5\n",
    "w = tf.Variable(tf.constant(5, dtype = tf.float32))\n",
    "\n",
    "# 定义损失函数loss\n",
    "loss = tf.square(w+1)\n",
    "\n",
    "# 定义反向传播\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# 生成Session，进行40轮训练\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print((\"After %s steps: w is %f, loss is %f.\") % (i, w_val, loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率设置多少合适？  \n",
    "使用指数衰减学习率，可以根据BATCH_SIZE的轮数动态更新学习率  \n",
    "![](https://raw.githubusercontent.com/king0682/FigureBed/master/img/20190718233645.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 steps: global_step is 1.000000, learning rate is 0.099000, loss is 77.440002\n",
      "After 1 steps: global_step is 2.000000, learning rate is 0.098010, loss is 49.809719\n",
      "After 2 steps: global_step is 3.000000, learning rate is 0.097030, loss is 32.196201\n",
      "After 3 steps: global_step is 4.000000, learning rate is 0.096060, loss is 20.912704\n",
      "After 4 steps: global_step is 5.000000, learning rate is 0.095099, loss is 13.649124\n",
      "After 5 steps: global_step is 6.000000, learning rate is 0.094148, loss is 8.950812\n",
      "After 6 steps: global_step is 7.000000, learning rate is 0.093207, loss is 5.897362\n",
      "After 7 steps: global_step is 8.000000, learning rate is 0.092274, loss is 3.903603\n",
      "After 8 steps: global_step is 9.000000, learning rate is 0.091352, loss is 2.595742\n",
      "After 9 steps: global_step is 10.000000, learning rate is 0.090438, loss is 1.733887\n",
      "After 10 steps: global_step is 11.000000, learning rate is 0.089534, loss is 1.163375\n",
      "After 11 steps: global_step is 12.000000, learning rate is 0.088638, loss is 0.784033\n",
      "After 12 steps: global_step is 13.000000, learning rate is 0.087752, loss is 0.530691\n",
      "After 13 steps: global_step is 14.000000, learning rate is 0.086875, loss is 0.360760\n",
      "After 14 steps: global_step is 15.000000, learning rate is 0.086006, loss is 0.246287\n",
      "After 15 steps: global_step is 16.000000, learning rate is 0.085146, loss is 0.168846\n",
      "After 16 steps: global_step is 17.000000, learning rate is 0.084294, loss is 0.116236\n",
      "After 17 steps: global_step is 18.000000, learning rate is 0.083451, loss is 0.080348\n",
      "After 18 steps: global_step is 19.000000, learning rate is 0.082617, loss is 0.055765\n",
      "After 19 steps: global_step is 20.000000, learning rate is 0.081791, loss is 0.038859\n",
      "After 20 steps: global_step is 21.000000, learning rate is 0.080973, loss is 0.027186\n",
      "After 21 steps: global_step is 22.000000, learning rate is 0.080163, loss is 0.019094\n",
      "After 22 steps: global_step is 23.000000, learning rate is 0.079361, loss is 0.013462\n",
      "After 23 steps: global_step is 24.000000, learning rate is 0.078568, loss is 0.009528\n",
      "After 24 steps: global_step is 25.000000, learning rate is 0.077782, loss is 0.006769\n",
      "After 25 steps: global_step is 26.000000, learning rate is 0.077004, loss is 0.004827\n",
      "After 26 steps: global_step is 27.000000, learning rate is 0.076234, loss is 0.003454\n",
      "After 27 steps: global_step is 28.000000, learning rate is 0.075472, loss is 0.002481\n",
      "After 28 steps: global_step is 29.000000, learning rate is 0.074717, loss is 0.001789\n",
      "After 29 steps: global_step is 30.000000, learning rate is 0.073970, loss is 0.001294\n",
      "After 30 steps: global_step is 31.000000, learning rate is 0.073230, loss is 0.000940\n",
      "After 31 steps: global_step is 32.000000, learning rate is 0.072498, loss is 0.000684\n",
      "After 32 steps: global_step is 33.000000, learning rate is 0.071773, loss is 0.000500\n",
      "After 33 steps: global_step is 34.000000, learning rate is 0.071055, loss is 0.000367\n",
      "After 34 steps: global_step is 35.000000, learning rate is 0.070345, loss is 0.000270\n",
      "After 35 steps: global_step is 36.000000, learning rate is 0.069641, loss is 0.000199\n",
      "After 36 steps: global_step is 37.000000, learning rate is 0.068945, loss is 0.000148\n",
      "After 37 steps: global_step is 38.000000, learning rate is 0.068255, loss is 0.000110\n",
      "After 38 steps: global_step is 39.000000, learning rate is 0.067573, loss is 0.000082\n",
      "After 39 steps: global_step is 40.000000, learning rate is 0.066897, loss is 0.000061\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "LEARNING_RATE_BASE = 0.1 # 最初学习率\n",
    "LEARNING_RATE_DECAY = 0.99 # 学习率衰减率\n",
    "LEARNING_RATE_STEP = 1 # 指喂入多少轮BATCH_SIZE后，更新一次学习率，一般设置为：总样本数/BATCH_SIZE\n",
    "\n",
    "# 定义BATCH_SIZE训练轮数计数器，初值为0，该变量设置为不可训练\n",
    "global_step = tf.Variable(0, trainable = False)\n",
    "\n",
    "# 定义指数下降学习率\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase=True)\n",
    "\n",
    "# 定义待优化参数w。初值为10\n",
    "w = tf.Variable(tf.constant(10, dtype = tf.float32))\n",
    "\n",
    "# 定义损失函数\n",
    "loss = tf.square(w+1)\n",
    "\n",
    "# 定义反向传播\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "# 生成Session，训练40轮\n",
    "with  tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        global_step_val = sess.run(global_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print(\"After %s steps: global_step is %f, learning rate is %f, loss is %f\" % (i, global_step_val, learning_rate_val, loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 滑动平均 ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
