{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B站：北大TensorFlow笔记\n",
    "\n",
    "## 目标1：\n",
    "搭建神经网络，总结搭建的八股\n",
    "### TensorFlow中的神经网络\n",
    "\n",
    "#### 整个计算过程\n",
    "* 张量表示数据\n",
    "* 计算图搭建神经网络\n",
    "* 会话执行计算图\n",
    "* 优化线上权重（参数）\n",
    "* 得到模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量表示数据\n",
    "1.先用`constant`构建两个常数向量的变量  \n",
    "2.两个向量做加法，返回值\n",
    "\n",
    "关于相应的名称\n",
    "* 乘法：mul\n",
    "* 除法：truediv\n",
    "* 加法：add\n",
    "* 减法：sub\n",
    "* 取余数：mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mod:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.0, 2.0])\n",
    "b = tf.constant([3.0, 4.0])\n",
    "result = a % b\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算图（Graph）搭建神经网路\n",
    "\n",
    "**能print()出以下内容的变量都称为计算图，即有加减乘除内容的运算**  \n",
    "`Tensor(\"mod:0\", shape=(2,), dtype=float32)`\n",
    "\n",
    "* 计算图参数的解读：  \n",
    "    * 张量名称`add:0`\n",
    "        * add->节点名  \n",
    "        * 0->第0个输出  \n",
    "    * shape->维度（2,）表示一维数组，长度为2  \n",
    "    * dtype=float32->数据类型\n",
    "\n",
    "**图只负责搭建网络，不进行计算**\n",
    "\n",
    "**矩阵相乘案例**\n",
    "\n",
    "* 一个中括号内有几个数，就代表有几列数\n",
    "* 一个中括号内有几个中括号，就代表有几行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_6:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.0, 2.0]])\n",
    "w = tf.constant([[3.0], [4.0]])\n",
    "y = tf.matmul(x, w)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用会话（Session）进行计算，执行计算图\n",
    "使用with实现，结构如下\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(1, 1), dtype=float32)\n",
      "[[ 11.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.0, 2.0]])\n",
    "w = tf.constant([[3.0], [4.0]])\n",
    "y = tf.matmul(x, w)\n",
    "print(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化权重（参数）\n",
    "\n",
    "**1.首先随机生成参数`tf.Variable()`**  \n",
    "`w = tf.Variable(tf.random_normal([2,3], stddev=2, mean=0, seed=1))`\n",
    "\n",
    "* tf.random_normal：生成正态分布的随机数\n",
    "    * 大小[2,3]\n",
    "    * 以下没有特殊要求可以不写：\n",
    "        * stddev：标准差=2\n",
    "        * mean：均值为0\n",
    "        * seed：随机种子为1（表示做随机数的序列，固定随机种子，每次生成的随机数保持一致）\n",
    "\n",
    "**2.关于生成随机数的分布，有以下几种：**  \n",
    "* tf.random_normal()：正态分布\n",
    "* tf.truncated_normal()：去掉过大偏离点的正态分布（如果随机出的数据超出平均值两个标准差，则会被去掉）\n",
    "* tf.random_uniform()：平均分布\n",
    "\n",
    "**3.生成数组常量**\n",
    "* tf.zeros，生成全0数组，`tf.zeros([3,2],int32)`，生成三行两列全0矩阵\n",
    "* tf.ones，生成全1数组，`tf.ones([3,2],int32)`，生成三行两列全1矩阵\n",
    "* tf.fill，生成全定值数组，`tf.fill([3,2],6)`，生成三行两列全1矩阵\n",
    "* tf.constant，根据给定的数值生成数组（前面用到过）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 6]\n",
      " [6 6]\n",
      " [6 6]]\n"
     ]
    }
   ],
   "source": [
    "q = tf.zeros([3,2], tf.int32)\n",
    "q = tf.ones([3,2], tf.int32)\n",
    "q = tf.fill([3,2], 6)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络的实现\n",
    "\n",
    "**整个过程：**\n",
    "\n",
    "* 训练模型  \n",
    "1.准备数据集，提取特征，作为输入喂给NN  \n",
    "2.从输入到输出，搭建NN结构（前向传播）：先搭建计算图，再执行会话  \n",
    "3.将特征数据喂给NN，迭代优化NN权值参数（反向传播） \n",
    "\n",
    "\n",
    "* 使用模型  \n",
    "4.将训练好的模型进行预测和分类  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建前向传播\n",
    "\n",
    "#### 变量初始化\n",
    "* 1.定义变量初值（搭建框架）\n",
    "初始化所有变量的函数，简写为初始化节点`init_op`\n",
    "```\n",
    "x = tf.constant([[0.7, 0.5]])\n",
    "w1 = tf.Variable(tf.random_normal([2, 3],stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1],stddev=1, seed=1))\n",
    "```\n",
    "* 2.执行初始化（赋予变量初值）\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "```\n",
    "\n",
    "#### 张量乘法\n",
    "* 1.定义张量之间的关系（搭建框架）\n",
    "```\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "```\n",
    "* 2.执行乘法\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    print(\"y is:\\n\", sess.run(y))\n",
    "```\n",
    "\n",
    "\n",
    "#### Other Points\n",
    "1.参数矩阵的具体形状  \n",
    "* 本层指的是计算层（W1是指第一个计算层，W2指的是第二层，因此不包含输入层）  \n",
    "* (上层节点个数, 本层节点个数）\n",
    "\n",
    "2.当前为什么没有用到激活函数和偏置值b？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is:\n",
      " [[ 3.0904665]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 两层全连接神经网络demo\n",
    "\n",
    "# 1.定义输入、随机正态分布初始化权重参数的初值\n",
    "# 定义一个固定输入，使用tf.constant\n",
    "x = tf.constant([[0.7, 0.5]])\n",
    "w1 = tf.Variable(tf.random_normal([2, 3],stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1],stddev=1, seed=1))\n",
    "\n",
    "# 2.定义前向传播过程中变量之间的关系（相乘）\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 3.使用Session计算结果\n",
    "with tf.Session() as sess:\n",
    "    # 01.初始化变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 02.进行NN计算，并且输出结果\n",
    "    print(\"y is:\\n\", sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 构建反向传播\n",
    "\n",
    "#### 喂数据\n",
    "先用`tf.placeholder`进行占位，shape=(数据个数, 数据维度)，告诉`sess.run(y)`要输入多少个数据\n",
    "\n",
    "* 喂一组数据，数据有两个特征：\n",
    "先占一个位置的数据，然后输入数据\n",
    "```\n",
    "x = tf.placeholder(tf.float32,shape=(1, 2))\n",
    "sess.run(y, feed_dict={x:[[0.5, 0.6]]})\n",
    "```\n",
    "\n",
    "* 喂多组数据，数据同样是有两维特征：\n",
    "数据个数填写None，代表无数个数据/未知\n",
    "```\n",
    "x = tf.placeholder(tf.float32,shape=(None, 2))\n",
    "sess.run(y, feed_dict={x:[[0.1,0.2], [0.2,0.3], [0.3,0.4], [0.4,0.5]]})\n",
    "```\n",
    "\n",
    "##### 向NN喂入一组特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is:\n",
      " [[ 3.0904665]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 两层全连接神经网络demo\n",
    "\n",
    "# 1.定义输入、随机正态分布初始化权重参数的初值\n",
    "# 定义一个输入，使用tf.placeholder()\n",
    "x = tf.placeholder(tf.float32, shape=(1, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3],stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1],stddev=1, seed=1))\n",
    "\n",
    "# 2.定义前向传播过程中变量之间的关系（相乘）\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 3.使用Session计算结果\n",
    "with tf.Session() as sess:\n",
    "    # 01.初始化变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 02.使用feed_dict喂入参数，进行NN计算，并且输出结果\n",
    "    print(\"y is:\\n\", sess.run(y, feed_dict={x: [[0.7, 0.5]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### 向NN喂入多组特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is:\n",
      " [[ 3.0904665 ]\n",
      " [ 1.2236414 ]\n",
      " [ 1.72707319]\n",
      " [ 2.23050475]]\n",
      "w1:\n",
      " [[-0.81131822  1.48459876  0.06532937]\n",
      " [-2.4427042   0.0992484   0.59122431]]\n",
      "w2:\n",
      " [[-0.81131822]\n",
      " [ 1.48459876]\n",
      " [ 0.06532937]]\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# 两层全连接神经网络demo\n",
    "\n",
    "# 1.定义输入、随机正态分布初始化权重参数的初值\n",
    "# 定义多个输入，使用tf.placeholder()\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3],stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1],stddev=1, seed=1))\n",
    "\n",
    "# 2.定义前向传播过程中变量之间的关系（相乘）\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 3.使用Session计算结果\n",
    "with tf.Session() as sess:\n",
    "    # 01.初始化变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 02.使用feed_dict喂入参数，进行NN计算，并且输出结果\n",
    "    print(\"y is:\\n\", sess.run(y, feed_dict={x: [[0.7, 0.5], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5]]}))\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 损失函数\n",
    "常见的损失函数\n",
    "* MSE均方误差\n",
    "即为预测值与真实值差的平方，除以样本个数n  \n",
    "`loss = tf.reduce_mean(tf.square(y_-y))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 反向传播训练\n",
    "目标：减小loss数值  \n",
    "常见方法：\n",
    "* 梯度下降  \n",
    "`train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)`\n",
    "* Momentum优化  \n",
    "`train_step = tf.train.MomentumOptimizer(learining_rate, momentum).minimize(loss)`\n",
    "* Adam  \n",
    "`train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)`\n",
    "\n",
    "其中，学习率learning_rate代表参数每次更新的幅度\n",
    "\n",
    "**代码案例如下：需要背下来**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch_size：一次喂给神经网络多少数据（此数值不可以过大，否则会吃不消）\n",
    "BATCH_SIZE = 8\n",
    "seed = 23455\n",
    "\n",
    "# 基于随机种子23455生成数据集\n",
    "rng = np.random.RandomState(seed)\n",
    "# 生成32行数据，每组数据都有 体积和重量 两个属性作为特征\n",
    "X = rng.rand(32,2)\n",
    "Y = [[int(x0 + x1 < 1)] for (x0, x1) in X]\n",
    "\n",
    "print(\"X:\\n\",X)\n",
    "print(\"Y:\\n\",Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 2.搭建NN的输入、输出、参数\n",
    "* 3.搭建NN的前向传播过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建NN的输入、输出、参数（其中y是矩阵计算后的值，而y_是从矩阵Y中取出来的标签\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_= tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 搭建NN的前向传播过程\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 4.反向传播，指定损失函数loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 均方误差计算损失\n",
    "loss = tf.reduce_mean(tf.square(y-y_))\n",
    "\n",
    "# 梯度下降开始学习，学习率0.001\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "train_step = tf.train.MomentumOptimizer(0.001, 0.9).minimize(loss)\n",
    "# train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成Session，训练steps轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[-0.81131822  1.48459876  0.06532937]\n",
      " [-2.4427042   0.0992484   0.59122431]]\n",
      "w2:\n",
      " [[-0.81131822]\n",
      " [ 1.48459876]\n",
      " [ 0.06532937]]\n",
      "\n",
      "\n",
      "After 1 training steps, loss_mse on all data is 5.13118\n",
      "After 501 training steps, loss_mse on all data is 0.384391\n",
      "After 1001 training steps, loss_mse on all data is 0.383592\n",
      "After 1501 training steps, loss_mse on all data is 0.383562\n",
      "After 2001 training steps, loss_mse on all data is 0.383561\n",
      "After 2501 training steps, loss_mse on all data is 0.383561\n",
      "After 3001 training steps, loss_mse on all data is 0.383561\n",
      "After 3501 training steps, loss_mse on all data is 0.383561\n",
      "After 4001 training steps, loss_mse on all data is 0.383561\n",
      "After 4501 training steps, loss_mse on all data is 0.383561\n",
      "After 5001 training steps, loss_mse on all data is 0.383561\n",
      "After 5501 training steps, loss_mse on all data is 0.383561\n",
      "After 6001 training steps, loss_mse on all data is 0.383561\n",
      "After 6501 training steps, loss_mse on all data is 0.383561\n",
      "After 7001 training steps, loss_mse on all data is 0.383561\n",
      "After 7501 training steps, loss_mse on all data is 0.383561\n",
      "After 8001 training steps, loss_mse on all data is 0.383561\n",
      "After 8501 training steps, loss_mse on all data is 0.383561\n",
      "After 9001 training steps, loss_mse on all data is 0.383561\n",
      "After 9501 training steps, loss_mse on all data is 0.383561\n",
      "After 10001 training steps, loss_mse on all data is 0.383561\n",
      "After 10501 training steps, loss_mse on all data is 0.383561\n",
      "After 11001 training steps, loss_mse on all data is 0.383561\n",
      "After 11501 training steps, loss_mse on all data is 0.383561\n",
      "After 12001 training steps, loss_mse on all data is 0.383561\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 初始化变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 输出训练前的权重\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # 训练模型\n",
    "    STEPS = 12001\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        # 分批喂入训练数据，进行权重的学习（梯度下降法）\n",
    "        sess.run(train_step, feed_dict={x:X[start:end], y_:Y[start:end]})\n",
    "#         print(\"train_step\\n\",train_step)\n",
    "        \n",
    "        # 输出训练后的权值参数\n",
    "#         print(\"\\n\")\n",
    "#         print(\"w1:\\n\",sess.run(w1))\n",
    "#         print(\"w2:\\n\",sess.run(w2))\n",
    "        \n",
    "        # 每500次计算一次均方误差\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss, feed_dict={x:X, y_:Y})\n",
    "            print(\"After %d training steps, loss_mse on all data is %g\" % (i+1, total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
